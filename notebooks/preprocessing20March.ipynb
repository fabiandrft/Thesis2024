{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "080bf8fb-6c69-4723-9cc5-d8e59a97899e",
   "metadata": {},
   "source": [
    "# Contents\n",
    "1. open csv, merge, check and remove duplicates\n",
    "2. ensure the right chats were scraped\n",
    "3. check for and remove duplicates and NA in text column\n",
    "4. remove chats that are not part of Querdenken community (mainstream media)\n",
    "5. apply classification of party, media, movement\n",
    "6. create message_id\n",
    "7. check user/chats relation per category of chat, remove duplicate channels due to case sensitivity\n",
    "8. investigate hashtags\n",
    "9. check datetime, create cutoff point (1 Jan 2020)\n",
    "10. check views\n",
    "11. preparing edgelist for network analysis\n",
    "12. dropping unnecessary columns, create 'text_clean'\n",
    "13. clean text with script (URL, lowercase, special characters)\n",
    "14. create subset and remove barely seen messages\n",
    "15. remove very short messages\n",
    "16. investigate short messages and grouping possibilities\n",
    "17. save new df_textanal_short to csv\n",
    "\n",
    "**end**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f06243-f349-4d6f-9cc9-a2d3870e999c",
   "metadata": {},
   "source": [
    "### step 1: Open csv, merge, check and remove duplicates\n",
    "from the three iterations of scraping (s1,s2,s3), the output csv are merged and checked for duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862b94d1-778c-4c7c-a544-4ecd01da22de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths to your CSV files\n",
    "csv_file_paths = ['output_s1.csv', 'output_s2.csv', 'output_s3_t5.csv']\n",
    "\n",
    "# Read the CSV files into DataFrames\n",
    "df1 = pd.read_csv(csv_file_paths[0])\n",
    "df2 = pd.read_csv(csv_file_paths[1])\n",
    "df3 = pd.read_csv(csv_file_paths[2])\n",
    "\n",
    "# 1. Check if all column names of the three CSVs are identical\n",
    "columns_identical = (df1.columns == df2.columns).all() and (df2.columns == df3.columns).all()\n",
    "\n",
    "# 2. See how many rows each CSV has\n",
    "rows_df1 = len(df1)\n",
    "rows_df2 = len(df2)\n",
    "rows_df3 = len(df3)\n",
    "\n",
    "# 3. Merge the three df into a new one called 'merged_df'\n",
    "merged_df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "\n",
    "# 4. Check if there are rows that are identical (duplicates) in the new csv\n",
    "duplicates_in_merged = merged_df.duplicated().any()\n",
    "\n",
    "# Output the results\n",
    "print(f\"Are all column names identical across the three CSVs? {columns_identical}\")\n",
    "print(f\"Number of rows in CSV 1: {rows_df1}\")\n",
    "print(f\"Number of rows in CSV 2: {rows_df2}\")\n",
    "print(f\"Number of rows in CSV 3: {rows_df3}\")\n",
    "print(f\"Are there any duplicate rows in 'output_final_t5.csv'? {duplicates_in_merged}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159cf6cc-0951-41c0-aa62-98d4e6ff6dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_duplicates = merged_df.duplicated(keep='first').sum()\n",
    "print(f\"Number of duplicate rows: {number_of_duplicates}\")\n",
    "merged_df = merged_df.drop_duplicates()\n",
    "print(\"duplicate rows have been removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71951944-d16d-45ef-a184-c44f6cec2e8d",
   "metadata": {},
   "source": [
    "Now check for duplicates in 'link_to_message' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb5e46f-cff3-4ca7-b468-93b9fcd4d797",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = merged_df.duplicated(subset='link_to_message', keep=False)\n",
    "\n",
    "# Counting the number of duplicates\n",
    "number_of_duplicates = duplicates.sum()\n",
    "\n",
    "# Printing the number of duplicates\n",
    "print(f\"Number of duplicate entries in 'link_to_message': {number_of_duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1d9624-669a-4fd7-9977-2c10aa0e710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop_duplicates(subset= \"link_to_message\", keep='first')\n",
    "duplicates = merged_df.duplicated(subset='link_to_message', keep=False)\n",
    "\n",
    "# Counting the number of duplicates\n",
    "number_of_duplicates = duplicates.sum()\n",
    "\n",
    "# Printing the number of duplicates\n",
    "print(f\"Number of duplicate entries in 'link_to_message': {number_of_duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6fe60b-7af0-468c-990d-943e70e578de",
   "metadata": {},
   "source": [
    "now the dataframe merged_df does not contain duplicate messages. The text of some messages may still be identical, but messages that were scraped multiple times (in the different seeds) have been removed (through removing duplicates of the unique links to the messages)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b04cd2-a1ff-42db-9e59-6aeddd795379",
   "metadata": {},
   "source": [
    "### step 2: check that only chats from the seeds were scraped\n",
    "qualitative assessment of those that are not from seeds: Retain y/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13474458-9c1e-4d06-b4b1-12d393a71dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9184589f-f531-4155-a85f-c9b096e598c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load channel names from the first .txt file\n",
    "with open('/home/onyxia/work/new/seeds/seed2.txt', 'r') as file:\n",
    "    channels_list_1 = [line.strip() for line in file]\n",
    "\n",
    "# Load channel names from the second .txt file\n",
    "with open('/home/onyxia/work/new/seeds/seed3_t5.txt', 'r') as file:\n",
    "    channels_list_2 = [line.strip() for line in file]\n",
    "combined_channel_list = list(set(channels_list_1 + channels_list_2))\n",
    "unmatched_channels = merged_df[~merged_df['channel_name'].isin(combined_channel_list)]\n",
    "print(\"Unmatched channel names:\")\n",
    "print(unmatched_channels['channel_name'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f6f583-8275-4f39-a63b-f3364ac4545c",
   "metadata": {},
   "source": [
    "Qualitative Assessment of the Chats: belong the movement/party. So they will be kept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587170bb-32ff-4325-a3c2-d8616d114f17",
   "metadata": {},
   "source": [
    "### step 3: check for and remove duplicates and NA in text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d674b1-1943-4518-bc25-41c6663b5bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 remove na in text column\n",
    "# Count rows where 'text' is NA\n",
    "na_count = merged_df['text'].isna().sum()\n",
    "print(f\"Number of rows with NA in 'text' column: {na_count}\")\n",
    "\n",
    "# Remove rows where 'text' is NA\n",
    "df_clean = merged_df.dropna(subset=['text']).copy()\n",
    "\n",
    "# Verify removal\n",
    "print(f\"Number of rows after removing NAs: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ce72b0-ba00-42bb-b7ec-417529d66daf",
   "metadata": {},
   "source": [
    "### step 4: remove chats from mainstream media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5330d65-bb9c-416a-879d-8db904eb3b63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#3. remove what is not part of Querdenken - check for keywords of mainstream media or main parties in 'channel_name'\n",
    "media_keywords = [\n",
    "    \"ARD\", \"ZDF\", \"Deutschlandfunk\", \"NDR\", \"WDR\", \n",
    "    \"SWR\", \"BR\", \"MDR\", \"rbb\", \"DW\", \n",
    "    \"Die Zeit\", \"FAZ\", \"Frankfurter Allgemeine\", \"Süddeutsche Zeitung\", \n",
    "    \"Der Spiegel\", \"Stern\", \"Focus\", \"Bild\", \"SPD\", \"FDP\", \"CDU\", \"AFD\", \"Linke\", \"Grüne\"\n",
    "]\n",
    "\n",
    "media_keywords_lower = [keyword.lower() for keyword in media_keywords]\n",
    "\n",
    "def contains_media_keyword(channel_name):\n",
    "    channel_name_lower = channel_name.lower() if isinstance(channel_name, str) else \"\"\n",
    "    return any(keyword in channel_name_lower for keyword in media_keywords_lower)\n",
    "    \n",
    "media_related_channel_names = [channel for channel in df_clean['channel_name'] if contains_media_keyword(channel)]\n",
    "\n",
    "media_related_channel_names_unique = list(set(media_related_channel_names))\n",
    "\n",
    "print(\"Unique channel names containing keywords related to German mainstream media:\")\n",
    "for channel in media_related_channel_names_unique:\n",
    "    print(channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edd4d51-bc05-4304-ae59-e62c2b8bb71f",
   "metadata": {},
   "source": [
    "no mainstream media channels detected based on keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae478885-7f9c-4b1b-9b42-0e9bd3e50684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6df836d-0f24-4586-9c9e-65067fe6fce3",
   "metadata": {},
   "source": [
    "### step 5: apply classification of media/movement/party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18df4bae-4d33-489e-9321-888169b2e18b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#categorise channels into party, movement, media\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame creation (replace this with your actual df_clean DataFrame)\n",
    "# df_clean = pd.DataFrame({\n",
    "#     'channel_name': ['basisSomething', 'InfoMediaChannel', 'TVChannel', 'MixedBasisTV']\n",
    "# })\n",
    "\n",
    "# Initialize 'channel_category' with default value 0\n",
    "df_clean['channel_category'] = 0\n",
    "\n",
    "# Function to categorize channel based on 'channel_name'\n",
    "def categorize_channel(channel_name):\n",
    "    channel_name_lower = channel_name.lower()\n",
    "    if 'basis' in channel_name_lower:\n",
    "        return 'party'\n",
    "    elif any(keyword in channel_name_lower for keyword in ['tv', 'info', 'media', 'news', 'magazin']):\n",
    "        return 'media'\n",
    "    return 'movement'  # Return 'movement'\n",
    "\n",
    "# Apply the categorization function to the 'channel_name' column\n",
    "df_clean['channel_category'] = df_clean['channel_name'].apply(categorize_channel)\n",
    "\n",
    "# Display the DataFrame to verify the new 'channel_category' column\n",
    "print(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049d6952-d948-4956-8483-216338dd5894",
   "metadata": {},
   "source": [
    "### step 6: add message_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573048e5-7102-48d2-849c-e5176eaa05d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create id for each message\n",
    "df_clean['message_id'] = df_clean.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b269836-45b9-4da0-a5fe-cb81d49a69ed",
   "metadata": {},
   "source": [
    "### step 7: check user/chats relation per category of chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a801d5be-f00c-4025-8317-9889e5d11e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 user column\n",
    "unique_users = df_clean['user'].nunique()\n",
    "print(f\"Number of unique users: {unique_users}\")\n",
    "\n",
    "# Count how many messages each user sent\n",
    "messages_per_user = df_clean['user'].value_counts()\n",
    "\n",
    "print(\"Messages sent per user (top users):\")\n",
    "print(messages_per_user.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3736dcf1-180c-4806-9714-6c39e472cfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = df_clean[df_clean['channel_name'] == df_clean['user']]\n",
    "matches_count = len(matches)\n",
    "print(f\"Number of cases where 'channel_name' equals 'user': {matches_count}\")\n",
    "matches = df_clean[df_clean['channel_name'] == df_clean['user']]\n",
    "matches_count_per_category = matches.groupby('channel_category').size()\n",
    "print(matches_count_per_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97250376-5f75-4adb-bbdb-4c1a62fd53dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "partei_users = set(df_clean[df_clean['channel_category'] == 'Partei']['user'].unique())\n",
    "media_users = set(df_clean[df_clean['channel_category'] == 'Media']['user'].unique())\n",
    "other_users = set(df_clean[df_clean['channel_category'] == '0']['user'].unique())\n",
    "# Identify overlaps between each pair of categories\n",
    "overlap_partei_media = partei_users.intersection(media_users)\n",
    "overlap_partei_other = partei_users.intersection(other_users)\n",
    "overlap_media_other = media_users.intersection(other_users)\n",
    "\n",
    "# Identify users present in all three categories\n",
    "overlap_all_three = partei_users.intersection(media_users, other_users)\n",
    "print(f\"Overlap between three categories {overlap_all_three}\")\n",
    "print(f\"Overlap between Partei and media {overlap_partei_media}\")\n",
    "print(f\"Overlap between Partei and Movement {overlap_partei_other}\")\n",
    "print(f\"Overlap between Media and Movement {overlap_media_other}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528d3bab-557a-46d3-bc92-bf4b653d9424",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#clean how many channels have been mentioned multiple times due to different capitalisations\n",
    "df_clean['channel_name_lower'] = df_clean['channel_name'].str.lower()\n",
    "\n",
    "# Count the unique, case-insensitive 'channel_name' occurrences\n",
    "unique_channels_case_insensitive = df_clean['channel_name_lower'].nunique()\n",
    "\n",
    "# Count the original, case-sensitive 'channel_name' occurrences\n",
    "unique_channels_case_sensitive = df_clean['channel_name'].nunique()\n",
    "\n",
    "# Calculate the difference to see how many are affected by case sensitivity\n",
    "difference = unique_channels_case_sensitive - unique_channels_case_insensitive\n",
    "\n",
    "print(f\"Unique channels (case-sensitive): {unique_channels_case_sensitive}\")\n",
    "print(f\"Unique channels (case-insensitive): {unique_channels_case_insensitive}\")\n",
    "print(f\"Number of channels affected by case sensitivity: {difference}\")\n",
    "# Group by the lowercase channel name and count the unique case-sensitive names for each\n",
    "channels_with_multiple_capitalizations = df_clean.groupby('channel_name_lower')['channel_name'].nunique()\n",
    "\n",
    "# Filter for groups with more than one unique case-sensitive name\n",
    "channels_with_multiple_capitalizations = channels_with_multiple_capitalizations[channels_with_multiple_capitalizations > 1]\n",
    "\n",
    "print(f\"Channels with multiple capitalizations: {len(channels_with_multiple_capitalizations)}\")\n",
    "print(channels_with_multiple_capitalizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaaa943-33a8-463c-8cf2-983b6c4576f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make 'channel_name' and 'user' lowercase\n",
    "df_clean['channel_name_lower'] = df_clean['channel_name'].str.lower()\n",
    "df_clean['user_lower'] = df_clean['user'].str.lower()\n",
    "\n",
    "# Count unique values in the original and lowercase columns for 'user'\n",
    "unique_users_case_sensitive = df_clean['user'].nunique()\n",
    "unique_users_case_insensitive = df_clean['user_lower'].nunique()\n",
    "\n",
    "# Calculate the difference to see how many are affected by case sensitivity for 'user'\n",
    "difference_users = unique_users_case_sensitive - unique_users_case_insensitive\n",
    "\n",
    "print(f\"Unique users (case-sensitive): {unique_users_case_sensitive}\")\n",
    "print(f\"Unique users (case-insensitive): {unique_users_case_insensitive}\")\n",
    "print(f\"Number of users affected by case sensitivity: {difference_users}\")\n",
    "\n",
    "# Drop the original 'channel_name' and 'user' columns\n",
    "df_clean = df_clean.drop(['channel_name', 'user'], axis=1)\n",
    "\n",
    "# Rename the lowercase columns to the original column names\n",
    "df_clean = df_clean.rename(columns={'channel_name_lower': 'channel_name', 'user_lower': 'user'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55338505-ebb2-428c-bbad-01310fe839c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_clean.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcb1f3d-7d29-4034-8e60-ed8e858116a5",
   "metadata": {},
   "source": [
    "### step 8: investigate hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5a61a9-d853-45a0-a600-14500e6945a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check hashtags\n",
    "temp_df = (df_clean.dropna(subset=['hashtags'])\n",
    "           .assign(individual_hashtags=lambda x: x['hashtags'].str.split(','))\n",
    "           .explode('individual_hashtags'))\n",
    "\n",
    "# Clean up individual hashtags: strip whitespace and convert to lowercase\n",
    "temp_df['individual_hashtags'] = temp_df['individual_hashtags'].str.strip().str.lower()\n",
    "\n",
    "# Group by 'channel_category' and 'individual_hashtags' and count frequencies\n",
    "hashtag_counts = (temp_df.groupby(['channel_category', 'individual_hashtags'])\n",
    "                  .size()\n",
    "                  .reset_index(name='count'))\n",
    "\n",
    "# Step 3: Sort within each 'channel_category' to identify the most frequent hashtags\n",
    "sorted_hashtag_counts = hashtag_counts.sort_values(by=['channel_category', 'count'], ascending=[True, False])\n",
    "\n",
    "# display top N hashtags for each category\n",
    "top_n = 5\n",
    "top_hashtags_by_category = sorted_hashtag_counts.groupby('channel_category').head(top_n)\n",
    "\n",
    "print(top_hashtags_by_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd785a1-e9e9-473d-8b5f-ff5704b1d392",
   "metadata": {},
   "source": [
    "### step 9: check time window and generate cutoff point (1 Jan 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b380de5-b767-49c0-99a8-2528a46f35ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check time window\n",
    "df_clean['datetime'] = pd.to_datetime(df_clean['datetime'])\n",
    "\n",
    "min_datetime = df_clean['datetime'].min()\n",
    "max_datetime = df_clean['datetime'].max()\n",
    "\n",
    "print(f\"Earliest datetime in the dataset: {min_datetime}\")\n",
    "print(f\"Latest datetime in the dataset: {max_datetime}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6f16fa-a651-405e-b183-86bc238d5892",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_datetime = pd.Timestamp('2020-03-01', tz='UTC')\n",
    "\n",
    "# Now you can safely compare and count messages sent before March 2020\n",
    "messages_before_march_2020 = df_clean[df_clean['datetime'] < cutoff_datetime].shape[0]\n",
    "\n",
    "print(f\"Number of messages sent before March 2020: {messages_before_march_2020}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741a861d-4395-46e1-bb48-513b8c0786f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_datetime_jan_2020 = pd.Timestamp('2020-01-01', tz='UTC')\n",
    "\n",
    "# Keep only messages on or after January 2020\n",
    "df_clean = df_clean[df_clean['datetime'] >= cutoff_datetime_jan_2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4b98a4-aca0-4188-b374-23ac131875c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check all went well\n",
    "min_datetime = df_clean['datetime'].min()\n",
    "max_datetime = df_clean['datetime'].max()\n",
    "\n",
    "print(f\"Earliest datetime in the dataset: {min_datetime}\")\n",
    "print(f\"Latest datetime in the dataset: {max_datetime}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f4d8f0-bcc2-4e32-a42d-5e6c69d36eda",
   "metadata": {},
   "source": [
    "### step 10: inspect views and create thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41e4193-aad4-4f27-9f7c-ebf09faaf4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated thresholds and labels to match the desired ranges\n",
    "thresholds = [100000, 10000, 1000, 100, 10]\n",
    "threshold_labels = ['100000>', '10000-99999', '1000-9999', '100-999', '10-99']\n",
    "\n",
    "# Updated function to categorize views into thresholds\n",
    "def categorize_views(views):\n",
    "    if views > 100000:\n",
    "        return '100000>'\n",
    "    elif views > 10000:\n",
    "        return '10000-99999'\n",
    "    elif views > 1000:\n",
    "        return '1000-9999'\n",
    "    elif views > 100:\n",
    "        return '100-999'\n",
    "    elif views > 10:\n",
    "        return '10-99'\n",
    "    else:\n",
    "        return '0-10'\n",
    "\n",
    "# Apply the updated categorization function to the 'views' column\n",
    "df_clean['view_threshold'] = df_clean['views'].apply(categorize_views)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e11e07-5513-498a-9b58-879e6b87000d",
   "metadata": {},
   "source": [
    "### step 11: create edgelist for network analysis; \n",
    "both edgelist(containing all connections even to chats outside the community, and edgelist_clean which replaces chats not in the community with 'other_chats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b6b26e-1275-4247-802a-41c37ee5d2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_clean.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e723361-01e6-463e-99d5-53b26f6c8176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gephi df_network as csv with the cleaned datafile\n",
    "import re\n",
    "df_network = df_clean[['channel_name', 'links', 'channel_category']].copy()\n",
    "def extract_valid_telegram_username(link_text):\n",
    "    # Ensure input is a string\n",
    "    if not isinstance(link_text, str):\n",
    "        return None\n",
    "    # Define the pattern for extracting Telegram usernames\n",
    "    telegram_pattern = re.compile(r'(?:https?://)?t\\.me/([\\w\\d_-]+)')\n",
    "    # Search for all occurrences of the pattern\n",
    "    matches = telegram_pattern.findall(link_text)\n",
    "    # Filter out any non-Telegram links or invalid entries\n",
    "    valid_usernames = [match for match in matches if not any(ext in match for ext in ['http', 'https', '|'])]\n",
    "    # Return the first valid Telegram username, if available\n",
    "    return valid_usernames[0] if valid_usernames else None\n",
    "\n",
    "# Apply the refined function to extract valid Telegram usernames\n",
    "df_network['links'] = df_network['links'].apply(extract_valid_telegram_username)\n",
    "df_network['links'] = df_network['links'].str.lower()\n",
    "\n",
    "# Drop rows without a valid Telegram username\n",
    "df_network = df_network.dropna(subset=['links'])\n",
    "df_network = df_network.dropna(subset=['channel_name'])\n",
    "df_network = df_network.dropna(subset=['channel_category'])\n",
    "\n",
    "contains_floats = df_network['links'].apply(lambda x: isinstance(x, float)).any()\n",
    "\n",
    "if contains_floats:\n",
    "    print(\"The column contains float values.\")\n",
    "    df_network = df_network[~df_network['links'].apply(lambda x: isinstance(x, float))]\n",
    "else:\n",
    "    print(\"No float values in 'links'.\")\n",
    "\n",
    "# create edgelist for copy\n",
    "edgelist = df_network.copy()\n",
    "edgelist['source'] = edgelist['channel_name'].str.lower()\n",
    "edgelist['target'] = edgelist['links'].str.lower()\n",
    "edgelist['channel_category'] = edgelist['channel_category'].str.lower()\n",
    "\n",
    "edgelist = edgelist.drop('channel_name', axis=1)\n",
    "edgelist = edgelist.drop('links', axis=1)\n",
    "# Display the first few rows of edgelist to verify\n",
    "print(edgelist.head())\n",
    "len(edgelist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62715464-98e2-4257-8a4c-4a31d3dbacf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read the text file, convert to lowercase, and split into a list\n",
    "with open('/path/to/seed3_t5.txt', 'r') as file:\n",
    "    valid_targets = file.read().lower().splitlines()\n",
    "\n",
    "# Step 2: Copy edgelist to edgelist_clean and convert 'target' in edgelist_clean to lowercase for comparison\n",
    "edgelist_clean = edgelist.copy()\n",
    "edgelist_clean['target'] = edgelist_clean['target'].str.lower()\n",
    "\n",
    "# Step 3: Replace values in 'target' that are not in valid_targets with 'other_chats'\n",
    "edgelist_clean['target'] = edgelist_clean['target'].apply(lambda x: x if x in valid_targets else 'other_chats')\n",
    "\n",
    "# Note: Convert 'target' back to its original case if needed here\n",
    "\n",
    "# Step 4: Your edgelist_clean is ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7776ed4b-794b-45b3-90e0-bf2175a4e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist_clean.to_csv(\"path/to/edgelist_clean_v2.csv\", index=False)\n",
    "edgelist.to_csv(\"path/to/edgelist_v2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbfce96-de9d-45a3-8534-e716eecdbb76",
   "metadata": {},
   "source": [
    "edgelist has been created. NA will continue in gephi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108e877f-d96d-4a44-9d04-50c73dffd2e5",
   "metadata": {},
   "source": [
    "### step 12: drop unnecessary columns, create 'text_clean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522c087f-4302-4583-b0a0-8d07b8b38042",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_clean.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51884c08-e9ac-4d1e-b2e3-cb0b41d7fea9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Columns to be dropped\n",
    "columns_to_drop = [\n",
    "    'user_img', 'nb_hashtags', 'hashtags', \n",
    "    'is_reply_img', 'is_reply_user', 'is_reply_text', \n",
    "    'stickers', 'nb_photos', 'photos', \n",
    "    'nb_videos', 'videos', 'videos_times', \n",
    "    'link_img', 'link_site', 'link_title', 'link_description', \n",
    "    'edited'\n",
    "]\n",
    "# Dropping the specified columns from df_clean\n",
    "df_clean = df_clean.drop(columns=columns_to_drop)\n",
    "\n",
    "# Displaying the first few rows to verify the columns have been dropped\n",
    "print(df_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f3aa73-e128-42c8-8c8b-92529c33130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_clean.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ae5bcf-0fcb-4d54-aab8-a947b69e8529",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['text_clean'] = df_clean['text']\n",
    "print(df_clean.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c86378c-dc6f-4028-abd6-769c794c307c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_order = [\n",
    "    'message_id', 'channel_name', 'channel_category', \n",
    "    'text', 'text_clean', 'views', 'view_threshold', \n",
    "    'datetime'\n",
    "]\n",
    "\n",
    "# Adding the remaining columns to the new order, ensuring no column is dropped\n",
    "remaining_columns = [col for col in df_clean.columns if col not in new_order]\n",
    "new_order += remaining_columns\n",
    "\n",
    "# Reassigning the columns of df_clean based on the new order\n",
    "df_clean = df_clean[new_order]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1257695c-d40c-432c-a728-8416a8ee21d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_clean.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e811a32-6e1b-4051-95bd-b9164d2fa4d7",
   "metadata": {},
   "source": [
    "### step 13: clean text with script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af17a65-42cc-43c3-9d68-e85c81219290",
   "metadata": {},
   "source": [
    "the script removes \"...\" multiple periods, converts to lowercase, removes URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd81c40f-dab1-4907-8b00-22e8440c0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('path/to/scripts')\n",
    "from text_broom import text_broom\n",
    "from text_broom_chill import text_broom_chill\n",
    "# Apply text_broom_chill to text_notsoclean\n",
    "df_clean['text_clean'] = df_clean['text_clean'].apply(text_broom_chill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c654ce-91c1-4904-944c-c7269aad070b",
   "metadata": {},
   "source": [
    "### step 14: create subset for further + remove barely seen messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1347b23-fde9-48c2-b605-12d94f62bff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save old cleaned datafile 'df_clean' for future reference\n",
    "df_clean.to_csv(\"path/to/big_csv_20March.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21336d2-6711-4496-9723-5b82ccee6073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new dataframe df_textanal for BERT\n",
    "df_textanal = df_clean\n",
    "df_textanal['channel_category'] = df_textanal['channel_category'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1659f71b-8c09-474c-9e54-0ed40b3176c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values_count = df_textanal['view_threshold'].value_counts()\n",
    "\n",
    "# Find out how this distribution is different for the three categories in 'channel_category'\n",
    "category_distribution = df_textanal.groupby('channel_category')['view_threshold'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "(unique_values_count, category_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a17f5f-fcc4-4b59-a8cc-81a2978ec61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_percent = category_distribution.div(category_distribution.sum(axis=1), axis=0) * 100\n",
    "\n",
    "distribution_percent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2915cc-da0e-4959-aca0-402dd413aa45",
   "metadata": {},
   "source": [
    "For party chat messages(channel_category=party), ca. 50% were viewed by between 100 and 1000 people. another 30% were viewed by 10-100 people. Only 5% were viewed by more than 10.000 people.\n",
    "\n",
    "For media chat messages, (channel_category=media), ca. 50% were seen by more than 10.000 people. 15% by >100 and another 15% by less than 100.\n",
    "\n",
    "For Community chat messages, (channel_category=movement), 30% were seen by >10.000 people, 30% by more than 10.000 people, 25% by 100-10.000 people.\n",
    "\n",
    "I would suggest dropping all messages <100 views from movement and media chats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f236c4-8a93-4270-8671-9f7ebd73c17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep rows that satisfy one of the following conditions:\n",
    "# 1. The channel_category is 'party'\n",
    "# 2. The channel_category is 'movement' or 'media', but views are 100 or more\n",
    "df_textanal_short = df_textanal[\n",
    "    (df_textanal['channel_category'] == 'party') |\n",
    "    ((df_textanal['channel_category'].isin(['movement', 'media'])) & (df_textanal['views'] >= 100))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f35c14a-b7fd-4128-85da-8200c96465ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of rows in the original DataFrame\n",
    "original_row_count = len(df_textanal)\n",
    "\n",
    "# Calculate the number of rows in the filtered DataFrame\n",
    "filtered_row_count = len(df_textanal_short)\n",
    "\n",
    "# Calculate the number of rows dropped\n",
    "rows_dropped = original_row_count - filtered_row_count\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of rows dropped: {rows_dropped}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebd1f50-d858-4b97-87f4-8bb32d41ea86",
   "metadata": {},
   "source": [
    "### step 15: remove very short messages\n",
    "check distribution of message length and then decide which messages to retain. Goal: greater uniformity for BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f741400-f91b-4615-b825-b6cd56c2a73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "contains_floats = df_textanal['text_clean'].apply(lambda x: isinstance(x, float)).any()\n",
    "\n",
    "if contains_floats:\n",
    "    print(\"The column contains float values.\")\n",
    "    df_textanal = df_textanal[~df_textanal['text_clean'].apply(lambda x: isinstance(x, float))]\n",
    "else:\n",
    "    print(\"No float values in 'text_clean'.\")\n",
    "contains_floats = df_textanal_short['text_clean'].apply(lambda x: isinstance(x, float)).any()\n",
    "\n",
    "if contains_floats:\n",
    "    print(\"The column contains float values.\")\n",
    "    df_textanal_short = df_textanal_short[~df_textanal['text_clean'].apply(lambda x: isinstance(x, float))]\n",
    "else:\n",
    "    print(\"No float values in 'text_clean'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126587ab-95f4-45c4-865e-cfb609bb6101",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df_textanal_short exists and has a column 'text_lowernopunct'\n",
    "word_counts_per_cell = df_textanal_short['text_clean'].apply(lambda x: len(x.split()))\n",
    "\n",
    "bins = [0, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 130, 150, 170, 200, 250, float('inf')]\n",
    "labels = ['<5', '6-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100', '101-110', '111-130', '131-150', '151-170', '171-200', '201-250', '251+']\n",
    "\n",
    "word_count_categories = pd.cut(word_counts_per_cell, bins=bins, labels=labels, right=False)\n",
    "word_count_distribution = word_count_categories.value_counts().sort_index()\n",
    "\n",
    "print(word_count_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f9e292-0bd1-4cb6-a7fd-372425fdec8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define bins and labels\n",
    "bins = [0, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 130, 150, 170, 200, 250, float('inf')]\n",
    "labels = ['<5', '6-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100', '101-110', '111-130', '131-150', '151-170', '171-200', '201-250', '251+']\n",
    "\n",
    "categories = [\"media\", \"movement\", \"party\"]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for category in categories:\n",
    "    # Filter the DataFrame for the current channel_category\n",
    "    df_subset = df_textanal_short[df_textanal_short['channel_category'] == category]\n",
    "    \n",
    "    # Calculate word count distribution for the subset\n",
    "    word_counts_per_cell = df_subset['text_clean'].apply(lambda x: len(x.split()))\n",
    "    word_count_categories = pd.cut(word_counts_per_cell, bins=bins, labels=labels, right=False)\n",
    "    word_count_distribution = word_count_categories.value_counts().sort_index()\n",
    "    \n",
    "    # Store results\n",
    "    results[category] = word_count_distribution\n",
    "\n",
    "# Now, results contains the word count distributions for each channel_category subset\n",
    "for category, distribution in results.items():\n",
    "    print(f\"Channel Category: {category}\")\n",
    "    print(distribution, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa97e7a7-01f5-42b0-ba18-2982121eb853",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for better visualisation: \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for category, distribution in results.items():\n",
    "    plt.plot(distribution.index, distribution.values, label=category)\n",
    "\n",
    "# Add labels and legend\n",
    "plt.title('Word Count Distribution by Channel Category')\n",
    "plt.xlabel('Word Count Categories')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.legend()\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap of labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5a107b-b158-45b1-8510-80072f06fe53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_textanal_short['word_count'] = df_textanal_short['text_clean'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Filter the DataFrame for rows where the number of words is less than 10\n",
    "df_less_than_10_words = df_textanal_short[df_textanal_short['word_count'] < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310cd262-d878-48eb-9e2f-dc2a911529ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_samples = df_less_than_10_words.sample(n=5)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# Display the sampled rows\n",
    "print(random_samples[['text_clean']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c90639-5e19-4b88-95b5-866fe1131ac6",
   "metadata": {},
   "source": [
    "may of the very short messages are currently na, which will be removed at a later stage.Those with less than 10 words are often quite meaningful. <5 words are not meaningful and can be removed right away from df_textanal_short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2b34a5-ae87-4dcb-b603-41c5b4ba8267",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_textanal_short = df_textanal_short[df_textanal_short['word_count'] >= 5]\n",
    "print(len(df_textanal))\n",
    "print(len(df_textanal_short))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b8fa60-6c36-4148-9c0e-a9c0d2282a74",
   "metadata": {},
   "source": [
    "### step 16: check if messages can be grouped together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f3bd83-1123-43eb-ac92-e47929a3ae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_textanal_short['datetime'] = pd.to_datetime(df_textanal_short['datetime'])\n",
    "\n",
    "# Sort the DataFrame by 'channel_name' and 'datetime'\n",
    "df_textanal_short.sort_values(by=['channel_name', 'datetime'], inplace=True)\n",
    "# Convert 'datetime' to just the date and time (without timezone) if not already\n",
    "df_textanal_short['datetime'] = df_textanal_short['datetime'].dt.tz_localize(None)\n",
    "\n",
    "# Initialize the 'thread' column with None or an appropriate default value\n",
    "df_textanal_short['thread'] = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd56a723-9d9e-499a-8732-1a74168b90a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now see average distance between messages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Filter the DataFrame for the channels of interest\n",
    "channels_of_interest = ['die_basis_funkt', 'zwanzig4', 'haintz']\n",
    "df_filtered = df_textanal_short[df_textanal_short['channel_name'].isin(channels_of_interest)]\n",
    "\n",
    "# Ensure datetime is in the correct format and sorted\n",
    "df_filtered['datetime'] = pd.to_datetime(df_filtered['datetime'])\n",
    "df_filtered.sort_values(by=['channel_name', 'datetime'], inplace=True)\n",
    "\n",
    "# Calculate time deltas within each channel\n",
    "df_filtered['time_delta'] = df_filtered.groupby('channel_name')['datetime'].diff().dt.total_seconds() / 60  # delta in minutes\n",
    "\n",
    "# Visualizing the distribution of time deltas\n",
    "for channel in channels_of_interest:\n",
    "    df_channel = df_filtered[df_filtered['channel_name'] == channel].dropna(subset=['time_delta'])\n",
    "    \n",
    "    # We use a logarithmic scale to better visualize the distribution, as it's likely to be heavily skewed\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(df_channel['time_delta'], bins=np.logspace(np.log10(1), np.log10(df_channel['time_delta'].max()), 50), color='skyblue', edgecolor='black')\n",
    "    plt.xscale('log')  # Set x-axis to logarithmic scale\n",
    "    plt.title(f'Log-scaled Distribution of Time Deltas for {channel}')\n",
    "    plt.xlabel('Time Delta (minutes, log scale)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, which=\"both\", ls=\"--\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88772a1-ce2f-4638-b865-8f251087aa5e",
   "metadata": {},
   "source": [
    "**It appears that the messages are rather more spread out - only in the case of haintz (a news channel) is the difference smaller. Given the great diversity of chats, finding a one-size-fits-all approach would be impossible. Therefore grouping messages into threads is not possible.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e724083d-dc2d-4e2e-b1e8-c7f6c3a96d24",
   "metadata": {},
   "source": [
    "### step 17: save as a new csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f940f862-4a8e-48dc-b346-853a5ca9fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_textanal_short.to_csv(\"path/to/textanal_short.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
